{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing ICESat-2 data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Select files of interest (segment and time)\n",
    "* Select area of interest (subset lon/lat)\n",
    "* Reduce selected files with variables of interest\n",
    "* Filter data and separate tracks into asc/des\n",
    "* Process/Read each file in parallel\n",
    "* Plot some data to check everything went well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How ICESat-2 files are organized \n",
    "\n",
    "ICESat-2 ground tracks are subsetted into granules (individual files)\n",
    "\n",
    "Granules are then grouped into latitudinal bands (segments)\n",
    "\n",
    "![Segments](segments.png \"Latitudinal bands (Segments)\")\n",
    "\n",
    "\n",
    "File naming convention:\n",
    "\n",
    "`ATL06_20181120202321_08130101_001_01.h5`\n",
    "\n",
    "`ATL06_[yyyymmdd][hhmmss]_[RGTccss]_[vvv_rr].h5`\n",
    "\n",
    "where\n",
    "\n",
    "`ATL_06` => L3A Land Ice product    \n",
    "\n",
    "`yyyymmdd` => Year, month, day of data acquisition    \n",
    "\n",
    "`hhmmss` => Hour, minute, second of data acquisition   \n",
    "\n",
    "`RGT` => Reference Ground Track    \n",
    "\n",
    "`cc` => Cycle Number   \n",
    "\n",
    "`ss` => Segment number (latitude band)   \n",
    "\n",
    "`vvv_rr` => Version and revision numbers  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select files of interest\n",
    "\n",
    "We will use the **file name** info for this (no need to open the files).  \n",
    "\n",
    "Alternatively, we could retrieve this info from the **Metadata**.\n",
    "\n",
    "To select files withint a time interval and segment, all we need is:\n",
    "\n",
    "`yyyymmdd, hhmmss, ss`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's firt get a list with all file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files: 0\n"
     ]
    }
   ],
   "source": [
    "#from utils import *\n",
    "\n",
    "def list_files_local(path):\n",
    "    \"\"\" Get file list form local folder. \"\"\"\n",
    "    from glob import glob\n",
    "    return glob(path)\n",
    "\n",
    "\n",
    "def list_files_ssh(path, host, user, pwd):\n",
    "    \"\"\" Get file list from remote folder usgin SSH. \"\"\"\n",
    "    import paramiko  # pip install paramiko\n",
    "\n",
    "    # Create an SSH client instance.\n",
    "    client = paramiko.SSHClient()\n",
    "\n",
    "    # Create a 'host_keys' object\n",
    "    # and load the local known hosts  \n",
    "    host_keys = client.load_system_host_keys()\n",
    "\n",
    "    # Connect to our client w/remote machine credentials\n",
    "    client.connect(host, username=user, password=pwd)\n",
    "\n",
    "    # Execute command on remote system,\n",
    "    # and get input, output and error variables\n",
    "    stdin, stdout, stderr = client.exec_command('ls '+path)\n",
    "\n",
    "    # Iterate over stdout\n",
    "    files = [line.strip('\\n') for line in stdout]\n",
    "\n",
    "    # Close the connection to client\n",
    "    client.close()\n",
    "    return files\n",
    "    \n",
    "    \n",
    "def list_files_s3(path):\n",
    "    \"\"\" Get file list from Amazon S3. \"\"\"\n",
    "    # code for Amazon S3 here\n",
    "    pass\n",
    "\n",
    "\n",
    "if 1:\n",
    "    files = list_files_local('data/*.h5')\n",
    "    \n",
    "elif 0:    \n",
    "    path = '/u/devon-r2/shared_data/icesat2/atl06/rel205/raw/*.h5'\n",
    "    host, user, pwd = 'devon.jpl.nasa.gov', 'paolofer', '********'\n",
    "\n",
    "    files = list_files_ssh(path, host, user, pwd)\n",
    "    \n",
    "else:\n",
    "    files = list_files_s3(path)\n",
    "    \n",
    "print('Total number of files:', len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter file names by segment and time interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "# File name format: ATL06_[yyyymmdd][hhmmss]_[RGTccss]_[vvv_rr].h5\n",
    "\n",
    "#NOTE: Need to simplify this function\n",
    "def time_from_fname(fname):\n",
    "    t = fname.split('_')[1]\n",
    "    y, m , d, h, mn, s = t[:4], t[4:6], t[6:8], t[8:10], t[10:12], t[12:14]\n",
    "    time = dt.datetime(int(y), int(m), int(d), int(h), int(mn), int(s))\n",
    "    return time\n",
    "\n",
    "\n",
    "def segment_from_fname(fname):\n",
    "    s = fname.split('_')[2]\n",
    "    return int(s[-2:])\n",
    "\n",
    "\n",
    "def select_files(files, segments=[10,11,12], t1=(2019,1,1), t2=(2019,2,1)):\n",
    "    t1 = dt.datetime(*t1)\n",
    "    t2 = dt.datetime(*t2)\n",
    "    files_out = []\n",
    "    for f in files:\n",
    "        fname = os.path.basename(f)\n",
    "        time = time_from_fname(fname)\n",
    "        segment = segment_from_fname(fname)\n",
    "        if t1 <= time <= t2 and segment in segments:\n",
    "            files_out.append(f)\n",
    "    return files_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/u/devon-r2/shared_data/icesat2/atl06/rel205/raw/ATL06_20190101001723_00540210_205_01.h5\n",
      "/u/devon-r2/shared_data/icesat2/atl06/rel205/raw/ATL06_20190101002504_00540211_205_01.h5\n",
      "/u/devon-r2/shared_data/icesat2/atl06/rel205/raw/ATL06_20190101003047_00540212_205_01.h5\n",
      "/u/devon-r2/shared_data/icesat2/atl06/rel205/raw/ATL06_20190101015140_00550210_205_01.h5\n",
      "/u/devon-r2/shared_data/icesat2/atl06/rel205/raw/ATL06_20190101015921_00550211_205_01.h5\n",
      "Number of files: 1388\n"
     ]
    }
   ],
   "source": [
    "files = select_files(files, segments=[10,11,12], t1=(2019,1,1), t2=(2019,2,1))\n",
    "\n",
    "for f in files[:5]: print(f)\n",
    "print('Number of files:', len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the selected files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for downloading selected files to -> data/\n",
    "# The files should already be dowloaded. This cell should not be executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce ICESat-2 files\n",
    "\n",
    "***\n",
    "**NOTE:** \n",
    "\n",
    "**This is neither the only nor the best way to handled ICESat-2 data files.**\n",
    "\n",
    "**This is *one* way that works well for large-scale processing (e.g. full continent) on parallel machines (e.g. HPC clusters).**\n",
    "\n",
    "**The idea is to (a) simplify the I/O of a complex workflow and (b) take advantage of embrrasingly parallelization.**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the ICESat-2 file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/*.h5: unable to open file\r\n"
     ]
    }
   ],
   "source": [
    "!h5ls -r data/*.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code a simple reader that:\n",
    "\n",
    "- Select variables of interest (x, y, t, h...)  \n",
    "- Filter data points based on quality flag and bbox   \n",
    "- Separate into beams and ascending/descending tracks  \n",
    "- Save data to a simpler HDF5 structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils import gps2dyr  <=== NOT WORKING!\n",
    "\n",
    "def read_atl06(ifile, bbox=None):\n",
    "    \"\"\" \n",
    "    Read 1 ATL06 file and output 12 reduced files. \n",
    "    \n",
    "    Extract variables of interest and separate the ATL06 file \n",
    "    into each beam (ground track) and ascending/descending orbits.\n",
    "    \"\"\"\n",
    "\n",
    "    # Each beam is a group\n",
    "    group = ['./gt1l','./gt1r','./gt2l','./gt2r','./gt3l','./gt3r']\n",
    "\n",
    "    # Loop trough beams\n",
    "    for k,g in enumerate(group):\n",
    "    \n",
    "        #-----------------------------------#\n",
    "        # 1) Read in data for a single beam #\n",
    "        #-----------------------------------#\n",
    "    \n",
    "        # Load variables into memory (more can be added!)\n",
    "        with h5py.File(ifile, 'r') as fi:\n",
    "            lat = fi[g+'/land_ice_segments/latitude'][:]\n",
    "            lon = fi[g+'/land_ice_segments/longitude'][:]\n",
    "            h_li = fi[g+'/land_ice_segments/h_li'][:]\n",
    "            s_li = fi[g+'/land_ice_segments/h_li_sigma'][:]\n",
    "            t_dt = fi[g+'/land_ice_segments/delta_time'][:]\n",
    "            flag = fi[g+'/land_ice_segments/atl06_quality_summary'][:]\n",
    "            s_fg = fi[g+'/land_ice_segments/fit_statistics/signal_selection_source'][:]\n",
    "            snr = fi[g+'/land_ice_segments/fit_statistics/snr_significance'][:]\n",
    "            h_rb = fi[g+'/land_ice_segments/fit_statistics/h_robust_sprd'][:]\n",
    "            dac = fi[g+'/land_ice_segments/geophysical/dac'][:]\n",
    "            f_sn = fi[g+'/land_ice_segments/geophysical/bsnow_conf'][:]\n",
    "            dh_fit_dx = fi[g+'/land_ice_segments/fit_statistics/dh_fit_dx'][:]\n",
    "            tide_earth = fi[g+'/land_ice_segments/geophysical/tide_earth'][:]\n",
    "            tide_load = fi[g+'/land_ice_segments/geophysical/tide_load'][:]\n",
    "            tide_ocean = fi[g+'/land_ice_segments/geophysical/tide_ocean'][:]\n",
    "            tide_pole = fi[g+'/land_ice_segments/geophysical/tide_pole'][:]\n",
    "            t_ref = fi['/ancillary_data/atlas_sdp_gps_epoch'][:]\n",
    "            rgt = fi['/orbit_info/rgt'][:] * np.ones(len(lat))\n",
    "\n",
    "        #---------------------------------------------#\n",
    "        # 2) Filter data according region and quality #\n",
    "        #---------------------------------------------#\n",
    "        \n",
    "        # Select a region of interest\n",
    "        if bbox:\n",
    "            lonmin, lonmax, latmin, latmax = bbox\n",
    "            bbox_mask = (lon >= lonmin) & (lon <= lonmax) & (lat >= latmin) & (lat <= latmax)\n",
    "        else:\n",
    "            bbox_mask = np.ones_like(lat, dtype=bool)  # get all\n",
    "            \n",
    "        # Only keep good data, and data inside bbox\n",
    "        mask = (flag == 0) & (np.abs(h_li) < 10e3) & (bbox_mask > 0)\n",
    "        \n",
    "        # Update variables\n",
    "        lat, lon, h_li, s_li, t_dt, h_rb, s_fg, snr, q_flag, f_sn, \\\n",
    "            tide_earth, tide_load, tide_ocean, tide_pole, dac, rgt = \\\n",
    "                lat[mask], lon[mask], h_li[mask],s_li[mask], t_dt[mask], h_rb[mask], \\\n",
    "                s_fg[mask], snr[mask], q_flag[mask], f_sn[mask], tide_earth[mask], \\\n",
    "                tide_load[mask], tide_ocean[mask], tide_pole[mask], dac[mask], rgt[mask]\n",
    "\n",
    "        # Test for no data\n",
    "        if len(h_li) == 0: return\n",
    "\n",
    "        #-------------------------------------#\n",
    "        # 3) Convert time and separate tracks #\n",
    "        #-------------------------------------#\n",
    "        \n",
    "        # Time in GPS seconds (secs sinde 1980...)\n",
    "        t_gps = t_ref + t_dt\n",
    "\n",
    "        # Time in decimal years\n",
    "        t_year = gps2dyr(t_gps)\n",
    "\n",
    "        # Determine orbit type\n",
    "        i_asc, i_des = track_type(t_year, lat)\n",
    "        \n",
    "        #-----------------------#\n",
    "        # 4) Save selected data #\n",
    "        #-----------------------#\n",
    "        \n",
    "        # Define output file name\n",
    "        ofile = ifile.replace('.h5', '_'+g[2:]+'.h5')\n",
    "        \n",
    "        #NOTE: Asc/Des can be saved in a single file w/index=0|1\n",
    "                \n",
    "        # Save ascending track data\n",
    "        if len(lat[i_asc]) > 1:\n",
    "            fasc = ofile.replace('.h5', '_A.h5')\n",
    "            with h5py.File(fasc, 'w') as fa:\n",
    "                fa['orbit'] = orb[i_asc][:]\n",
    "                fa['lon'] = lon[i_asc][:]\n",
    "                fa['lat'] = lat[i_asc][:]\n",
    "                fa['h_elv'] = h_li[i_asc][:]\n",
    "                fa['s_elv'] = s_li[i_asc][:]\n",
    "                fa['t_year'] = t_li[i_asc][:]\n",
    "                fa['h_rb'] = h_rb[i_asc][:]\n",
    "                fa['s_fg'] = s_fg[i_asc][:]\n",
    "                fa['snr'] = snr[i_asc][:]\n",
    "                fa['q_flg'] = q_flag[i_asc][:]\n",
    "                fa['f_sn'] = f_sn[i_asc][:]\n",
    "                fa['t_sec'] = t_gps[i_asc][:]\n",
    "                fa['tide_load'] = tide_load[i_asc][:]\n",
    "                fa['tide_ocean'] = tide_ocean[i_asc][:]\n",
    "                fa['tide_pole'] = tide_pole[i_asc][:]\n",
    "                fa['tide_earth'] = tide_earth[i_asc][:]\n",
    "                fa['dac'] = dac[i_asc][:]\n",
    "                fa['rgt'] = rgt[i_asc][:]                \n",
    "\n",
    "                print('asc ->', fasc)\n",
    "\n",
    "        # Save desending track data\n",
    "        if len(lat[i_des]) > 1:\n",
    "            fdes = ofile.replace('.h5', '_D.h5')\n",
    "            with h5py.File(fdes, 'w') as fd:\n",
    "                fd['orbit'] = orb[i_des][:]\n",
    "                fd['lon'] = lon[i_des][:]\n",
    "                fd['lat'] = lat[i_des][:]\n",
    "                fd['h_elv'] = h_li[i_des][:]\n",
    "                fd['s_li'] = s_li[i_des][:]\n",
    "                fd['t_year'] = t_li[i_des][:]\n",
    "                fd['h_rb'] = h_rb[i_des][:]\n",
    "                fd['s_fg'] = s_fg[i_des][:]\n",
    "                fd['snr'] = snr[i_des][:]\n",
    "                fd['q_flg'] = q_flag[i_des][:]\n",
    "                fd['f_sn'] = f_sn[i_des][:]\n",
    "                fd['t_sec'] = t_gps[i_des][:]\n",
    "                fd['tide_load'] = tide_load[i_des][:]\n",
    "                fd['tide_ocean'] = tide_ocean[i_des][:]\n",
    "                fd['tide_pole'] = tide_pole[i_des][:]\n",
    "                fd['tide_earth'] = tide_earth[i_des][:]\n",
    "                fd['dac'] = dac[i_des][:]\n",
    "                fd['rgt'] = rgt[i_des][:]\n",
    "                \n",
    "                print('des ->', fdes)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Merge beams for crossover analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running in parallel (16 jobs) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done   0 out of   0 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "njobs = 16\n",
    "\n",
    "files = [] #list_files_local(path)\n",
    "\n",
    "#NOTE: Using Kamb bounding box for now\n",
    "bbox = [-1124782, 81623, -919821, -96334]\n",
    "\n",
    "if njobs == 1:\n",
    "    print('running in serial ...')\n",
    "    [read_atl06(f, bbox) for f in files]\n",
    "\n",
    "else:\n",
    "    print('running in parallel (%d jobs) ...' % njobs)\n",
    "    from joblib import Parallel, delayed\n",
    "    Parallel(n_jobs=njobs, verbose=5)(delayed(read_atl06)(f, bbox) for f in files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our created files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: data/*.h5: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/*.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/*gt2r_A.h5: unable to open file\r\n"
     ]
    }
   ],
   "source": [
    "!h5ls -r data/*gt2r_A.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot some data to check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot points within region to check (map and scatter)\n",
    "* Plot tracks/beams to check (map and profiles)\n",
    "* Show single program with all of the above in one go (`readatl06.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
